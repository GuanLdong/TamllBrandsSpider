{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分代买主要来自崔庆才老师的《Python3网络爬虫代码》，我的工作就是改了一下Redis相关的代码，适应了新版本的语法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SCORE = 100\n",
    "MIN_SCORE = 0\n",
    "INITIAL_SCORE = 10\n",
    "REDIS_HOST = 'localhost'\n",
    "REDIS_PORT = 6379\n",
    "REDIS_PASSWORD = None\n",
    "REDIS_KEY = 'proxies'\n",
    "\n",
    "import redis\n",
    "from random import choice\n",
    "\n",
    "class RedisClient(object):\n",
    "    def __init__(self,host = REDIS_HOST,port = REDIS_PORT,password = REDIS_PASSWORD):\n",
    "        \"\"\"\n",
    "        Init\n",
    "        \"\"\"\n",
    "        self.db = redis.StrictRedis(host=host,port = port,password=password,decode_responses=True)\n",
    "        \n",
    "    def add(self,porxy,score=INITIAL_SCORE):\n",
    "        \"\"\"\n",
    "        add proxy,init score is 10\n",
    "        \"\"\"\n",
    "        if not self.db.zscore(REDIS_KEY,score,proxy):\n",
    "            return self.db.zadd(REDIS_KEY,score,proxy)\n",
    "        \n",
    "    def random(self):\n",
    "        \"\"\"\n",
    "        random choice high score proxy,if high score don't exist,choice next.\n",
    "        \"\"\"\n",
    "        result = self.zrangebyscore(REDIS_KEY, MAX_SCORE, MAX_SCORE)\n",
    "        if len(result):\n",
    "            return choice(result)\n",
    "        else:\n",
    "            result = self.db.zrevrange(REDIS_KEY,0,100)\n",
    "            if len(result):\n",
    "                return choice(result)\n",
    "            else:\n",
    "                raise PoolEmptyError\n",
    "    def decrease(self,proxy):\n",
    "        \"\"\"\n",
    "        proxy decrease 1 ,if score smaller than lowest,remove proxy.\n",
    "        \"\"\"\n",
    "        score = self.db.zscore(REDIS_KEY,proxy)\n",
    "        if score and score > MIN_SCORE:\n",
    "            print('PROXY : ',porxy,' NOW SCORE : ',score,' - 1' )\n",
    "            return self.db.zincrby(REDIS_KEY,proxy, -1)\n",
    "        else:\n",
    "            print('PROXY : ',proxy,' REMOVED')\n",
    "            return self.db.zrem(REDIS_KEY,proxy)\n",
    "        \n",
    "    def exist(self,proxy):\n",
    "        \"\"\"\n",
    "        if some proxy exist?\n",
    "        \"\"\"\n",
    "        return not self.db.zscore(REDIS_KEY,proxy) == None\n",
    "\n",
    "    def max(self,proxy):\n",
    "        \"\"\"\n",
    "        set max score\n",
    "        \"\"\"\n",
    "        print('PROXY : ',proxy,' CAN USE, SCORE IS ',MAX_SCORE)\n",
    "        return self.db.zadd(REDIS_KEY,MAX_SCORE,proxy)\n",
    "        \n",
    "    def count(self):\n",
    "        \"\"\"\n",
    "        GET Proxy number\n",
    "        \"\"\"\n",
    "        return self.db.zcard(REDIS_KEY)\n",
    "    \n",
    "    def all(self):\n",
    "        \"\"\"\n",
    "        GET ALL Proxy\n",
    "        \"\"\"\n",
    "        return self.db.zrangbyscore(REDIS_KEY,MIN_SCORE,MAX_SCORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "base_headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36',\n",
    "    'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "    'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7'\n",
    "}\n",
    "\n",
    "\n",
    "def get_page(url, options={}):\n",
    "    \"\"\"\n",
    "    抓取代理\n",
    "    :param url:\n",
    "    :param options:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    headers = dict(base_headers, **options)\n",
    "    print('正在抓取', url)\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        print('抓取成功', url, response.status_code)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "    except ConnectionError:\n",
    "        print('抓取失败', url)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pyquery import PyQuery as pq\n",
    "\n",
    "class ProxyMetaclass(type):\n",
    "    def __new__(cls, name, bases, attrs):\n",
    "        count = 0\n",
    "        attrs['__CrawlFunc__'] = []\n",
    "        for k, v in attrs.items():\n",
    "            if 'crawl_' in k:\n",
    "                attrs['__CrawlFunc__'].append(k)\n",
    "                count +=1\n",
    "        attrs['__CrawlFuncCount__'] = count\n",
    "        return type.__new__(cls,name,bases,attrs)\n",
    "\n",
    "class Crawler(object,metaclass=ProxyMetaclass):\n",
    "    def get_proxies(self, callback):\n",
    "        proxies = []\n",
    "        for proxy in eval(\"self.{}()\".format(callback)):\n",
    "            print(\"SUCCESS GET \",proxy)\n",
    "            proxies.append(proxy)\n",
    "        return proxies\n",
    "    \n",
    "    def crawl_daili66(self,page_count=4):\n",
    "        \"\"\"\n",
    "        GET daili66' s proxies\n",
    "        \"\"\"\n",
    "        start_url = 'http://www.66ip.cn/{}.html'\n",
    "        urls = [start_url.format(page) for page in range(1,page_count+1)]\n",
    "        for url in urls:\n",
    "            print('Crawling', url)\n",
    "            html = get_page(url)\n",
    "            if html:\n",
    "                doc = pq(html)\n",
    "                trs = doc('.containerbox table tr:gt(0)').items()\n",
    "                for tr in trs:\n",
    "                    ip = tr.find('td:nth-child(1)').text()\n",
    "                    port = tr.find('td:nth-child(2)').text()\n",
    "                    yield ':'.join([ip, port])\n",
    "    \n",
    "    def crawl_ip3366(self):\n",
    "        for page in range(1, 4):\n",
    "            start_url = 'http://www.ip3366.net/free/?stype=1&page={}'.format(page)\n",
    "            html = get_page(start_url)\n",
    "            ip_address = re.compile('<tr>\\s*<td>(.*?)</td>\\s*<td>(.*?)</td>')\n",
    "            # \\s * 匹配空格，起到换行作用\n",
    "            re_ip_address = ip_address.findall(html)\n",
    "            for address, port in re_ip_address:\n",
    "                result = address+':'+ port\n",
    "                yield result.replace(' ', '')\n",
    "\n",
    "    def crawl_kuaidaili(self):\n",
    "        for i in range(1, 4):\n",
    "            start_url = 'http://www.kuaidaili.com/free/inha/{}/'.format(i)\n",
    "            html = get_page(start_url)\n",
    "            if html:\n",
    "                ip_address = re.compile('<td data-title=\"IP\">(.*?)</td>') \n",
    "                re_ip_address = ip_address.findall(html)\n",
    "                port = re.compile('<td data-title=\"PORT\">(.*?)</td>')\n",
    "                re_port = port.findall(html)\n",
    "                for address,port in zip(re_ip_address, re_port):\n",
    "                    address_port = address+':'+port\n",
    "                    yield address_port.replace(' ','')\n",
    "    \n",
    "    def crawl_xicidaili(self):\n",
    "        for i in range(1, 3):\n",
    "            start_url = 'http://www.xicidaili.com/nn/{}'.format(i)\n",
    "            headers = {\n",
    "                'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "                'Cookie':'_free_proxy_session=BAh7B0kiD3Nlc3Npb25faWQGOgZFVEkiJWRjYzc5MmM1MTBiMDMzYTUzNTZjNzA4NjBhNWRjZjliBjsAVEkiEF9jc3JmX3Rva2VuBjsARkkiMUp6S2tXT3g5a0FCT01ndzlmWWZqRVJNek1WanRuUDBCbTJUN21GMTBKd3M9BjsARg%3D%3D--2a69429cb2115c6a0cc9a86e0ebe2800c0d471b3',\n",
    "                'Host':'www.xicidaili.com',\n",
    "                'Referer':'http://www.xicidaili.com/nn/3',\n",
    "                'Upgrade-Insecure-Requests':'1',\n",
    "            }\n",
    "            html = get_page(start_url, options=headers)\n",
    "            if html:\n",
    "                find_trs = re.compile('<tr class.*?>(.*?)</tr>', re.S)\n",
    "                trs = find_trs.findall(html)\n",
    "                for tr in trs:\n",
    "                    find_ip = re.compile('<td>(\\d+\\.\\d+\\.\\d+\\.\\d+)</td>') \n",
    "                    re_ip_address = find_ip.findall(tr)\n",
    "                    find_port = re.compile('<td>(\\d+)</td>')\n",
    "                    re_port = find_port.findall(tr)\n",
    "                    for address,port in zip(re_ip_address, re_port):\n",
    "                        address_port = address+':'+port\n",
    "                        yield address_port.replace(' ','')\n",
    "    \n",
    "    def crawl_iphai(self):\n",
    "        start_url = 'http://www.iphai.com/'\n",
    "        html = get_page(start_url)\n",
    "        if html:\n",
    "            find_tr = re.compile('<tr>(.*?)</tr>', re.S)\n",
    "            trs = find_tr.findall(html)\n",
    "            for s in range(1, len(trs)):\n",
    "                find_ip = re.compile('<td>\\s+(\\d+\\.\\d+\\.\\d+\\.\\d+)\\s+</td>', re.S)\n",
    "                re_ip_address = find_ip.findall(trs[s])\n",
    "                find_port = re.compile('<td>\\s+(\\d+)\\s+</td>', re.S)\n",
    "                re_port = find_port.findall(trs[s])\n",
    "                for address,port in zip(re_ip_address, re_port):\n",
    "                    address_port = address+':'+port\n",
    "                    yield address_port.replace(' ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db import RedisClient\n",
    "\n",
    "POOL_UPPER_THRESHOLD = 10000\n",
    "\n",
    "class Getter():"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
